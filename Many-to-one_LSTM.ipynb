{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-one LSTM\n",
    "\n",
    "ref: UCB-CS282-John F. Canny\n",
    "\n",
    "In this notebook we implement Many-to-One Long Short-Term Memory using a modular approach. For each layer we implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "  \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing LSTM networks of arbitrary depth, we also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from batchnormlstm.classifiers.lstm import *\n",
    "from batchnormlstm.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from batchnormlstm.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (982, 4, 28)\n",
      "y_val:  (982, 28)\n",
      "X_train:  (17758, 4, 28)\n",
      "y_train:  (17758, 28)\n",
      "X_test:  (983, 4, 28)\n",
      "y_test:  (983, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset and process it.\n",
    "import pandas as pd\n",
    "raw = pd.read_csv('.\\\\dataset\\\\energydata_complete.csv', index_col=0)\n",
    "\n",
    "T = 4\n",
    "test_pct = 0.05\n",
    "\n",
    "data = {'X_val': np.empty([int(test_pct * raw.index.size) - T, T, raw.columns.size]), \n",
    "        'y_val': np.empty([int(test_pct * raw.index.size) - T, raw.columns.size]), \n",
    "        'X_train': np.empty([int((1 - test_pct) * raw.index.size) - T \n",
    "                             - int(test_pct * raw.index.size), T, raw.columns.size]), \n",
    "        'y_train': np.empty([int((1 - test_pct) * raw.index.size) - T \n",
    "                             - int(test_pct * raw.index.size), raw.columns.size]), \n",
    "        'X_test': np.empty([raw.index.size - T \n",
    "                            - int((1 - test_pct) * raw.index.size), T, raw.columns.size]), \n",
    "        'y_test': np.empty([raw.index.size - T \n",
    "                            - int((1 - test_pct) * raw.index.size), raw.columns.size])}\n",
    "for i in range(int(test_pct * raw.index.size) - T):\n",
    "    data['X_val'][i] = raw.iloc[i:(i + T), :].values\n",
    "    data['y_val'][i] = raw.iloc[i + T, :].values\n",
    "for i in range(int(test_pct * raw.index.size), int((1 - test_pct) * raw.index.size) - T):\n",
    "    data['X_train'][i - int(test_pct * raw.index.size)] = raw.iloc[i:(i + T), :].values\n",
    "    data['y_train'][i - int(test_pct * raw.index.size)] = raw.iloc[i + T, :].values\n",
    "for i in range(int((1 - test_pct) * raw.index.size), raw.index.size - T):\n",
    "    data['X_test'][i - int((1 - test_pct) * raw.index.size)] = raw.iloc[i:(i + T), :].values\n",
    "    data['y_test'][i - int((1 - test_pct) * raw.index.size)] = raw.iloc[i + T, :].values\n",
    "\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: forward\n",
    "In file `batchnormlstm/layers.py` we implement the `affine_forward` function, then test the implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.769847728806635e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare the outputs. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Implement the `affine_backward` function and test the implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.580274371758088e-10\n",
      "dw error:  1.6327342070374637e-10\n",
      "db error:  8.79291994626051e-12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layers: forward\n",
    "Implement the forward pass for tanh and sigmoid activation function in the `tanh_forward` and `sigmoid_forward` function and test the implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tanh_forward function:\n",
      "difference:  3.8292287781296644e-08\n",
      "Testing sigmoid_forward function:\n",
      "difference:  5.157221295671855e-09\n"
     ]
    }
   ],
   "source": [
    "# Test the tanh_forward and sigmoid_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = tanh_forward(x)\n",
    "correct_out = np.array([[-0.46211716, -0.38770051, -0.30786199, -0.22343882],\n",
    "                        [-0.13552465, -0.04542327,  0.04542327,  0.13552465],\n",
    "                        [ 0.22343882,  0.30786199,  0.38770051,  0.46211716]])\n",
    "\n",
    "# Compare the outputs. The error should be around 1e-8\n",
    "print('Testing tanh_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))\n",
    "\n",
    "out, _ = sigmoid_forward(x)\n",
    "correct_out = np.array([[0.37754067, 0.39913012, 0.42111892, 0.44342513],\n",
    "                        [0.46596182, 0.48863832, 0.51136168, 0.53403818],\n",
    "                        [0.55657487, 0.57888108, 0.60086988, 0.62245933]])\n",
    "\n",
    "# Compare the outputs. The error should be around 1e-8\n",
    "print('Testing sigmoid_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation layers: backward\n",
    "Implement the backward pass for tanh and sigmoid activation function in the `tanh_backward` and `sigmoid_backward` function and test the implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tanh_backward function:\n",
      "dx error:  1.9348653675634142e-10\n",
      "Testing sigmoid_backward function:\n",
      "dx error:  1.0034653863721936e-10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(5, 5, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: tanh_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = tanh_forward(x)\n",
    "dx = tanh_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing tanh_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: sigmoid_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = sigmoid_forward(x)\n",
    "dx = sigmoid_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing sigmoid_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization: Forward\n",
    "In the file `batchnormlstm/layers.py`, we implement the batch normalization forward pass in the function `batchnorm_forward`, then run the following to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [-30.33501244 -30.97886183  38.33433603]\n",
      "  stds:  [36.32146779 31.83002163 31.20518914]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [-1.95399252e-16  5.77315973e-17 -8.19899704e-16]\n",
      "  std:  [1.         1.         0.99999999]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [11. 12. 13.]\n",
      "  stds:  [1.         1.99999999 2.99999998]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print('  means: ', a.mean(axis=0))\n",
    "print('  stds: ', a.std(axis=0))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
    "print('  mean: ', a_norm.mean(axis=0))\n",
    "print('  std: ', a_norm.std(axis=0))\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print('After batch normalization (nontrivial gamma, beta)')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [ 0.07693787 -0.02578972  0.05492385]\n",
      "  stds:  [0.99651805 1.04399964 0.95703145]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "for t in range(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization: backward\n",
    "Implement the backward pass for batch normalization in the function `batchnorm_backward`.\n",
    "\n",
    "To derive the backward pass we write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; sum gradients across these branches in the backward pass.\n",
    "\n",
    "Run the following to numerically check the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.8510779751549673e-10\n",
      "dgamma error:  5.7137851762636384e-12\n",
      "dbeta error:  3.3697549129496796e-11\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization: alternative backward\n",
    "We derive a simple expression for the batch normalization backward pass after working out derivatives on paper and simplifying. Implement the simplified batch normalization backward pass in the function `batchnorm_backward_alt` and compare the two implementations by running the following. Our two implementations should compute nearly identical results, but the alternative implementation should be a bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx difference:  3.14204955235961e-11\n",
      "dgamma difference:  0.0\n",
      "dbeta difference:  0.0\n",
      "speedup: 3.00x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print('dx difference: ', rel_error(dx1, dx2))\n",
    "print('dgamma difference: ', rel_error(dgamma1, dgamma2))\n",
    "print('dbeta difference: ', rel_error(dbeta1, dbeta2))\n",
    "print('speedup: %.2fx' % ((t2 - t1) / (t3 - t2)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout forward pass\n",
    "In the file `batchnormlstm/layers.py`, implement the forward pass for dropout. Since dropout behaves differently during training and testing, we implement the operation for both modes.\n",
    "\n",
    "Run the cell below to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests with p =  0.3\n",
      "Mean of input:  10.000280495638942\n",
      "Mean of train-time output:  10.003757970096457\n",
      "Mean of test-time output:  10.000280495638942\n",
      "Fraction of train-time output set to zero:  0.699864\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.6\n",
      "Mean of input:  10.000280495638942\n",
      "Mean of train-time output:  10.012958120128776\n",
      "Mean of test-time output:  10.000280495638942\n",
      "Fraction of train-time output set to zero:  0.399508\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.75\n",
      "Mean of input:  10.000280495638942\n",
      "Mean of train-time output:  10.012202300191028\n",
      "Mean of test-time output:  10.000280495638942\n",
      "Fraction of train-time output set to zero:  0.249144\n",
      "Fraction of test-time output set to zero:  0.0\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(500, 500) + 10\n",
    "rates = [0.3, 0.6, 0.75]\n",
    "\n",
    "for i, p in enumerate(rates):\n",
    "  out, _ = dropout_forward(x, {'mode': 'train', 'p': p})\n",
    "  out_test, _ = dropout_forward(x, {'mode': 'test', 'p': p})\n",
    "\n",
    "  print('Running tests with p = ', p)\n",
    "  print('Mean of input: ', x.mean())\n",
    "  print('Mean of train-time output: ', out.mean())\n",
    "  print('Mean of test-time output: ', out_test.mean())\n",
    "  print('Fraction of train-time output set to zero: ', (out == 0).mean())\n",
    "  print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "  if i < (len(rates) - 1):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout backward pass\n",
    "In the file `batchnormlstm/layers.py`, implement the backward pass for dropout. Run the following cell to numerically gradient-check the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx relative error:  5.4456072614148924e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dropout_param = {'mode': 'train', 'p': 0.8, 'seed': 123}\n",
    "out, cache = dropout_forward(x, dropout_param)\n",
    "dx = dropout_backward(dout, cache)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_forward(xx, dropout_param)[0], x, dout)\n",
    "\n",
    "print('dx relative error: ', rel_error(dx, dx_num))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Unit: forward\n",
    "Implement a LSTM forward pass for one time step with `lstm_forward_unit`.\n",
    "\n",
    "ref: https://blog.aidangomez.ca/2016/04/17/Backpropogating-an-LSTM-A-Numerical-Example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lstm_forward_unit function:\n",
      "difference:  3.5742222070529497e-09\n"
     ]
    }
   ],
   "source": [
    "# Test the lstm_forward_unit function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "inputX_size = num_inputs * np.prod(input_shape)\n",
    "weightW_size = 4 * output_dim * np.prod(input_shape)\n",
    "inputS_size = num_inputs * output_dim\n",
    "weightU_size = 4 * output_dim * output_dim\n",
    "scale_size = 4 * output_dim\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=inputX_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weightW_size).reshape(np.prod(input_shape), 4 * output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=scale_size)\n",
    "h_prev = np.linspace(0.1, -0.5, num=inputS_size).reshape(num_inputs, output_dim)\n",
    "c_prev = np.linspace(0.2, -0.3, num=inputS_size).reshape(num_inputs, output_dim)\n",
    "u = np.linspace(0.3, -0.1, num=weightU_size).reshape(output_dim, 4 * output_dim)\n",
    "\n",
    "out, _, _ = lstm_forward_unit(x, w, u, b, h_prev, c_prev)\n",
    "correct_out = np.array([[0.63273332, 0.60414966, 0.57096795],\n",
    "                        [0.67986212, 0.63027987, 0.57357252]])\n",
    "\n",
    "# Compare the outputs. The error should be around 1e-9.\n",
    "print('Testing lstm_forward_unit function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM & BatchNorm-LSTM Unit: backward\n",
    "Then implement the `lstm_backward_unit` and `batchnorm_lstm_backward_unit` function and numerically check the implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lstm_backward_unit function:\n",
      "dx error:  5.676013334485782e-10\n",
      "dw error:  9.09439438048634e-10\n",
      "du error:  6.022805979019028e-09\n",
      "db error:  2.958323064832232e-10\n",
      "\n",
      "Testing batchnorm_lstm_backward_unit function:\n",
      "dx error:  8.584173797425142e-09\n",
      "dw error:  4.5670732371827704e-08\n",
      "du error:  7.438542676015566e-08\n",
      "dgamma_x error:  3.725801103979747e-10\n",
      "dbeta_x error:  9.704859248215657e-10\n",
      "dgamma_h error:  1.6933380811135852e-08\n",
      "dbeta_h error:  1.0815590892310194e-09\n",
      "dgamma_c error:  5.595591165982771e-10\n",
      "dbeta_c error:  3.837078124809926e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the lstm_backward_unit function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 20)\n",
    "b = np.random.randn(20)\n",
    "h_prev = np.random.randn(10, 5)\n",
    "c_prev = np.random.randn(10, 5)\n",
    "u = np.random.randn(5, 20)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: lstm_forward_unit(x, w, u, b, h_prev, c_prev)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: lstm_forward_unit(x, w, u, b, h_prev, c_prev)[0], w, dout)\n",
    "du_num = eval_numerical_gradient_array(lambda u: lstm_forward_unit(x, w, u, b, h_prev, c_prev)[0], u, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: lstm_forward_unit(x, w, u, b, h_prev, c_prev)[0], b, dout)\n",
    "\n",
    "_, _, cache = lstm_forward_unit(x, w, u, b, h_prev, c_prev)\n",
    "dx, dw, du, db, _, _, _ = lstm_backward_unit(dout, cache)\n",
    "\n",
    "# The error should be around 1e-9\n",
    "print('Testing lstm_backward_unit function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('du error: ', rel_error(du_num, du))\n",
    "print('db error: ', rel_error(db_num, db))\n",
    "print()\n",
    "\n",
    "# Test the batchnorm_lstm_backward_unit function\n",
    "\n",
    "gamma_x = np.random.randn(20)\n",
    "gamma_h = np.random.randn(20)\n",
    "gamma_c = np.random.randn(5)\n",
    "beta_x = np.random.randn(20)\n",
    "beta_h = np.random.randn(20)\n",
    "beta_c = np.random.randn(5)\n",
    "bn_params = [{'mode': 'train'} for i in range(3)]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: \n",
    "                                       batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                   (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                   h_prev, c_prev)[0], \n",
    "                                       x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: \n",
    "                                       batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                   (beta_x, beta_h, beta_c), bn_params,\n",
    "                                                                   h_prev, c_prev)[0], \n",
    "                                       w, dout)\n",
    "du_num = eval_numerical_gradient_array(lambda u: \n",
    "                                       batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                   (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                   h_prev, c_prev)[0], \n",
    "                                       u, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: \n",
    "                                       batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                   (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                   h_prev, c_prev)[0], \n",
    "                                       b, dout)\n",
    "dgamma_x_num = eval_numerical_gradient_array(lambda gamma_x: \n",
    "                                             batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                         (beta_x, beta_h, beta_c), bn_params,\n",
    "                                                                         h_prev, c_prev)[0], \n",
    "                                             gamma_x, dout)\n",
    "dbeta_x_num = eval_numerical_gradient_array(lambda beta_x: \n",
    "                                            batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                        (beta_x, beta_h, beta_c), bn_params,\n",
    "                                                                        h_prev, c_prev)[0], \n",
    "                                            beta_x, dout)\n",
    "dgamma_h_num = eval_numerical_gradient_array(lambda gamma_h: \n",
    "                                             batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                         (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                         h_prev, c_prev)[0], \n",
    "                                             gamma_h, dout)\n",
    "dbeta_h_num = eval_numerical_gradient_array(lambda beta_h: \n",
    "                                            batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                        (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                        h_prev, c_prev)[0], \n",
    "                                            beta_h, dout)\n",
    "dgamma_c_num = eval_numerical_gradient_array(lambda gamma_c: \n",
    "                                             batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                         (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                         h_prev, c_prev)[0], \n",
    "                                             gamma_c, dout)\n",
    "dbeta_c_num = eval_numerical_gradient_array(lambda beta_c: \n",
    "                                            batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                                                        (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                        h_prev, c_prev)[0], \n",
    "                                            beta_c, dout)\n",
    "\n",
    "_, _, cache = batchnorm_lstm_forward_unit(x, w, u, b, (gamma_x, gamma_h, gamma_c), \n",
    "                                          (beta_x, beta_h, beta_c), bn_params, h_prev, c_prev)\n",
    "dx, dw, du, db, dgammas, dbetas, _, _, _ = batchnorm_lstm_backward_unit(dout, cache)\n",
    "\n",
    "dgamma_x, dgamma_h, dgamma_c = dgammas\n",
    "dbeta_x, dbeta_h, dbeta_c = dbetas\n",
    "\n",
    "# The error should be around 1e-9\n",
    "print('Testing batchnorm_lstm_backward_unit function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('du error: ', rel_error(du_num, du))\n",
    "print('dgamma_x error: ', rel_error(dgamma_x_num, dgamma_x))\n",
    "print('dbeta_x error: ', rel_error(dbeta_x_num, dbeta_x))\n",
    "print('dgamma_h error: ', rel_error(dgamma_h_num, dgamma_h))\n",
    "print('dbeta_h error: ', rel_error(dbeta_h_num, dbeta_h))\n",
    "print('dgamma_c error: ', rel_error(dgamma_c_num, dgamma_c))\n",
    "print('dbeta_c error: ', rel_error(dbeta_c_num, dbeta_c))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembled Layer: forward\n",
    "Implement a batch-normalized LSTM forward pass with `batchnorm_lstm_forward` that loops through `batchnorm_lstm_forward_unit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batchnorm_lstm_forward function:\n",
      "difference:  2.505371298335007e-07\n"
     ]
    }
   ],
   "source": [
    "# Test the batchnorm_lstm_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 5\n",
    "time_step = 3\n",
    "\n",
    "inputX_size = num_inputs * time_step * np.prod(input_shape)\n",
    "weightW_size = 4 * output_dim * np.prod(input_shape)\n",
    "inputS_size = num_inputs * output_dim\n",
    "weightU_size = 4 * output_dim * output_dim\n",
    "scale_size = 4 * output_dim\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=inputX_size).reshape(num_inputs, time_step, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weightW_size).reshape(np.prod(input_shape), 4 * output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=scale_size)\n",
    "h_n1 = np.linspace(0.1, -0.5, num=inputS_size).reshape(num_inputs, output_dim)\n",
    "c_n1 = np.linspace(0.2, -0.3, num=inputS_size).reshape(num_inputs, output_dim)\n",
    "u = np.linspace(0.3, -0.1, num=weightU_size).reshape(output_dim, 4 * output_dim)\n",
    "\n",
    "gamma_x = np.linspace(3., 4., num=scale_size)\n",
    "gamma_h = np.linspace(-2., -5., num=scale_size)\n",
    "gamma_c = np.linspace(1., 6., num=output_dim)\n",
    "beta_x = np.linspace(-0.1, 0.3, num=scale_size)\n",
    "beta_h = np.linspace(-0.2, 0.2, num=scale_size)\n",
    "beta_c = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "bn_params = [{'mode': 'train'} for i in range(3)]\n",
    "\n",
    "outs, _ = batchnorm_lstm_forward(x, w, u, b, (gamma_x, gamma_h, gamma_c),\n",
    "                                 (beta_x, beta_h, beta_c), bn_params)\n",
    "out = outs[:, -1, :]\n",
    "correct_out = np.array([[-0.61717064, -0.73806868, -0.77826679, -0.80687535, -0.83184207],\n",
    "                        [ 0.26743016,  0.41807813,  0.42096102,  0.41164257,  0.40158984]])\n",
    "\n",
    "# Compare the outputs. The error should be around 1e-7.\n",
    "print('Testing batchnorm_lstm_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembled Layer: backward\n",
    "Implement the `batchnorm_lstm_backward` function and test the implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lstm_backward_unit function:\n",
      "dx error:  6.510179435612519e-08\n",
      "dw error:  4.77448491604468e-07\n",
      "du error:  2.683146430218781e-07\n",
      "db error:  7.013542670085222e-09\n",
      "dgamma_x error:  4.715754374359772e-09\n",
      "dbeta_x error:  7.013542670085222e-09\n",
      "dgamma_h error:  3.0510325475552296e-09\n",
      "dbeta_h error:  7.247845978538795e-09\n",
      "dgamma_c error:  6.701361988181442e-10\n",
      "dbeta_c error:  2.216564624855941e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the batchnorm_lstm_backward function\n",
    "\n",
    "x = np.random.randn(10, 4, 2, 3)\n",
    "w = np.random.randn(6, 20)\n",
    "b = np.random.randn(20)\n",
    "h_n1 = np.random.randn(10, 5)\n",
    "c_n1 = np.random.randn(10, 5)\n",
    "u = np.random.randn(5, 20)\n",
    "dout = np.random.randn(10, 4, 5)\n",
    "\n",
    "gamma_x = np.random.randn(20)\n",
    "gamma_h = np.random.randn(20)\n",
    "gamma_c = np.random.randn(5)\n",
    "beta_x = np.random.randn(20)\n",
    "beta_h = np.random.randn(20)\n",
    "beta_c = np.random.randn(5)\n",
    "bn_params = [{'mode': 'train'} for i in range(3)]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: batchnorm_lstm_forward(x, w, u, b, (gamma_x, gamma_h, gamma_c),\n",
    "                                                                        (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                        h_n1, c_n1)[0], \n",
    "                                       x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: batchnorm_lstm_forward(x, w, u, b, (gamma_x, gamma_h, gamma_c),\n",
    "                                                                        (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                        h_n1, c_n1)[0], \n",
    "                                       w, dout)\n",
    "du_num = eval_numerical_gradient_array(lambda u: batchnorm_lstm_forward(x, w, u, b, (gamma_x, gamma_h, gamma_c),\n",
    "                                                                        (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                        h_n1, c_n1)[0], \n",
    "                                       u, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: batchnorm_lstm_forward(x, w, u, b, (gamma_x, gamma_h, gamma_c),\n",
    "                                                                        (beta_x, beta_h, beta_c), bn_params, \n",
    "                                                                        h_n1, c_n1)[0], \n",
    "                                       b, dout)\n",
    "dgamma_x_num = eval_numerical_gradient_array(lambda gamma_x: batchnorm_lstm_forward(x, w, u, b, \n",
    "                                                                                    (gamma_x, gamma_h, gamma_c),\n",
    "                                                                                    (beta_x, beta_h, beta_c), \n",
    "                                                                                    bn_params, h_n1, c_n1)[0], \n",
    "                                             gamma_x, dout)\n",
    "dbeta_x_num = eval_numerical_gradient_array(lambda beta_x: batchnorm_lstm_forward(x, w, u, b, \n",
    "                                                                                  (gamma_x, gamma_h, gamma_c),\n",
    "                                                                                  (beta_x, beta_h, beta_c), \n",
    "                                                                                  bn_params, h_n1, c_n1)[0], \n",
    "                                            beta_x, dout)\n",
    "dgamma_h_num = eval_numerical_gradient_array(lambda gamma_h: batchnorm_lstm_forward(x, w, u, b, \n",
    "                                                                                    (gamma_x, gamma_h, gamma_c),\n",
    "                                                                                    (beta_x, beta_h, beta_c), \n",
    "                                                                                    bn_params, h_n1, c_n1)[0], \n",
    "                                             gamma_h, dout)\n",
    "dbeta_h_num = eval_numerical_gradient_array(lambda beta_h: batchnorm_lstm_forward(x, w, u, b, \n",
    "                                                                                  (gamma_x, gamma_h, gamma_c),\n",
    "                                                                                  (beta_x, beta_h, beta_c), \n",
    "                                                                                  bn_params, h_n1, c_n1)[0], \n",
    "                                            beta_h, dout)\n",
    "dgamma_c_num = eval_numerical_gradient_array(lambda gamma_c: batchnorm_lstm_forward(x, w, u, b, \n",
    "                                                                                    (gamma_x, gamma_h, gamma_c),\n",
    "                                                                                    (beta_x, beta_h, beta_c), \n",
    "                                                                                    bn_params, h_n1, c_n1)[0], \n",
    "                                             gamma_c, dout)\n",
    "dbeta_c_num = eval_numerical_gradient_array(lambda beta_c: batchnorm_lstm_forward(x, w, u, b, \n",
    "                                                                                  (gamma_x, gamma_h, gamma_c),\n",
    "                                                                                  (beta_x, beta_h, beta_c), \n",
    "                                                                                  bn_params, h_n1, c_n1)[0], \n",
    "                                            beta_c, dout)\n",
    "\n",
    "_, cache = batchnorm_lstm_forward(x, w, u, b, (gamma_x, gamma_h, gamma_c), (beta_x, beta_h, beta_c), bn_params, \n",
    "                                  h_n1, c_n1)\n",
    "dx, dw, du, db, dgammas, dbetas = batchnorm_lstm_backward(dout, cache)\n",
    "\n",
    "dgamma_x, dgamma_h, dgamma_c = dgammas\n",
    "dbeta_x, dbeta_h, dbeta_c = dbetas\n",
    "\n",
    "# The error should be around 1e-7\n",
    "print('Testing lstm_backward_unit function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('du error: ', rel_error(du_num, du))\n",
    "print('db error: ', rel_error(db_num, db))\n",
    "print('dgamma_x error: ', rel_error(dgamma_x_num, dgamma_x))\n",
    "print('dbeta_x error: ', rel_error(dbeta_x_num, dbeta_x))\n",
    "print('dgamma_h error: ', rel_error(dgamma_h_num, dgamma_h))\n",
    "print('dbeta_h error: ', rel_error(dbeta_h_num, dbeta_h))\n",
    "print('dgamma_c error: ', rel_error(dgamma_c_num, dgamma_c))\n",
    "print('dbeta_c error: ', rel_error(dbeta_c_num, dbeta_c))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layer: MSE\n",
    "Implement the loss function in `batchnormlstm/layers.py`.\n",
    "\n",
    "We make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mse_loss:\n",
      "loss:  2.052333493108092\n",
      "dx error:  2.555032546004221e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_outputs, num_inputs = 10, 50\n",
    "x = np.random.randn(num_inputs, dim_outputs)\n",
    "y = np.random.randn(num_inputs, dim_outputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: mse_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = mse_loss(x, y)\n",
    "\n",
    "# Test mse_loss function. Loss should be around 2 and dx error should be 1e-7\n",
    "print('Testing mse_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer LSTM network\n",
    "\n",
    "In the file `batchnormlstm/classifiers/lstm.py` we complete the implementation of the `TwoLayerLSTM` class. This class will serve as a model for the other networks we implement. Run the cell below to test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization) ...\n",
      "Testing training loss (with regularization) ...\n",
      "Running numeric gradient check with reg =  0.0\n",
      "U relative error: 3.33e-04\n",
      "W1 relative error: 1.60e-03\n",
      "W2 relative error: 5.15e-10\n",
      "b1 relative error: 2.49e-04\n",
      "b2 relative error: 4.77e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "U relative error: 1.68e-05\n",
      "W1 relative error: 2.44e-05\n",
      "W2 relative error: 4.08e-07\n",
      "b1 relative error: 3.29e-03\n",
      "b2 relative error: 1.61e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N, D, H, O = 3, 5, 50, 6\n",
    "T = 4\n",
    "\n",
    "std = 1e-2\n",
    "model = TwoLayerLSTM(input_dim=D, hidden_dim=H, output_dim=O, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "U_std = abs(model.params['U'].std() - std)\n",
    "assert W1_std < std / 10, 'LSTM layer input weights do not seem right'\n",
    "assert np.all(b1 == 0), 'LSTM layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Affine layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Affine layer biases do not seem right'\n",
    "assert U_std < std / 10, 'LSTM layer state weights do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*4*H).reshape(D, 4*H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=4*H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*O).reshape(H, O)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=O)\n",
    "model.params['U'] = np.linspace(-0.5, 0.5, num=H*4*H).reshape(H, 4*H)\n",
    "X = np.linspace(-5.5, 4.5, num=N*T*D).reshape((D, T, N)).T\n",
    "h_prev = np.zeros((N, H))\n",
    "c_prev = np.zeros((N, H))\n",
    "\n",
    "outs = model.loss(X)\n",
    "correct_outs = np.asarray(\n",
    "  [[1.30180544, 1.61810112, 1.9343968,  2.25069248, 2.56698816, 2.88328384],\n",
    "   [1.30212866, 1.61834704, 1.93456542, 2.2507838,  2.56700219, 2.88322057],\n",
    "   [1.30257611, 1.61870838, 1.93484065, 2.25097292, 2.56710519, 2.88323746]])\n",
    "outs_diff = np.abs(outs - correct_outs).sum()\n",
    "assert outs_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization) ...')\n",
    "Y = np.linspace(2.5, -3.5, num=N*O).reshape(O, N).T\n",
    "loss, grads = model.loss(X, Y)\n",
    "correct_loss = 12.319904047979042\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "print('Testing training loss (with regularization) ...')\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, Y)\n",
    "correct_loss = 497.3609656985614\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, Y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, Y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "In the file `batchnormlstm/solver.py`, a modular designed class is implemented to train the models. Below we test a `Solver` instance by using it to train a `TwoLayerLSTM` with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 2136) loss: 22253.129490\n",
      "(Epoch 0 / 12) train_acc: -22185.110217; val_acc: -22781.851593\n",
      "(Iteration 101 / 2136) loss: 20145.900421\n",
      "(Epoch 1 / 12) train_acc: -19107.623912; val_acc: -19704.533949\n",
      "(Iteration 201 / 2136) loss: 19202.921033\n",
      "(Iteration 301 / 2136) loss: 17690.170766\n",
      "(Epoch 2 / 12) train_acc: -16626.700185; val_acc: -17288.975922\n",
      "(Iteration 401 / 2136) loss: 16180.004132\n",
      "(Iteration 501 / 2136) loss: 15364.115024\n",
      "(Epoch 3 / 12) train_acc: -14818.106415; val_acc: -15379.650053\n",
      "(Iteration 601 / 2136) loss: 14785.742143\n",
      "(Iteration 701 / 2136) loss: 13533.043745\n",
      "(Epoch 4 / 12) train_acc: -13375.693523; val_acc: -13850.797382\n",
      "(Iteration 801 / 2136) loss: 12911.198287\n",
      "(Epoch 5 / 12) train_acc: -12027.126239; val_acc: -12612.166069\n",
      "(Iteration 901 / 2136) loss: 12071.863752\n",
      "(Iteration 1001 / 2136) loss: 11343.273746\n",
      "(Epoch 6 / 12) train_acc: -11023.291617; val_acc: -11598.197938\n",
      "(Iteration 1101 / 2136) loss: 10664.458458\n",
      "(Iteration 1201 / 2136) loss: 10859.916089\n",
      "(Epoch 7 / 12) train_acc: -10369.438458; val_acc: -10760.392038\n",
      "(Iteration 1301 / 2136) loss: 9784.279122\n",
      "(Iteration 1401 / 2136) loss: 9523.482561\n",
      "(Epoch 8 / 12) train_acc: -9555.842451; val_acc: -10061.981881\n",
      "(Iteration 1501 / 2136) loss: 9729.767490\n",
      "(Iteration 1601 / 2136) loss: 8990.035079\n",
      "(Epoch 9 / 12) train_acc: -9006.586181; val_acc: -9475.180415\n",
      "(Iteration 1701 / 2136) loss: 8580.812757\n",
      "(Epoch 10 / 12) train_acc: -8420.525353; val_acc: -8978.760488\n",
      "(Iteration 1801 / 2136) loss: 8587.047197\n",
      "(Iteration 1901 / 2136) loss: 8204.087135\n",
      "(Epoch 11 / 12) train_acc: -8159.737744; val_acc: -8556.223861\n",
      "(Iteration 2001 / 2136) loss: 7714.433912\n",
      "(Iteration 2101 / 2136) loss: 7826.676022\n",
      "(Epoch 12 / 12) train_acc: -7747.826907; val_acc: -8194.753265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-7961.696083205202"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TwoLayerLSTM(input_dim=28, hidden_dim=10)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# Use a Solver instance to train a TwoLayerLSTM                              #\n",
    "##############################################################################\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                },\n",
    "                lr_decay=0.9,\n",
    "                num_epochs=12, batch_size=100,\n",
    "                print_every=100)\n",
    "solver.train(regress=True)\n",
    "solver.check_accuracy(data['X_test'], data['y_test'], regress=True)\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced LSTM network\n",
    "Next we implement the multi-layer LSTM network with an arbitrary number of hidden layers, and the options of batch normalization and dropout, as the `AdvancedLSTM` class in the file `batchnormlstm/classifiers/lstm.py`.\n",
    "\n",
    "ref: <br />\n",
    "https://arxiv.org/pdf/1603.09025.pdf <br />\n",
    "https://arxiv.org/pdf/1409.2329.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization and see if the initial losses seem reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  0.6364185128745645\n",
      "U1 relative error: 6.23e-04\n",
      "U2 relative error: 1.06e-03\n",
      "W1 relative error: 1.24e-04\n",
      "W2 relative error: 6.81e-04\n",
      "W3 relative error: 6.66e-05\n",
      "b1 relative error: 2.69e-05\n",
      "b2 relative error: 1.44e-05\n",
      "b3 relative error: 4.11e-11\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  36.14152720955063\n",
      "U1 relative error: 6.91e-07\n",
      "U2 relative error: 8.19e-07\n",
      "W1 relative error: 7.27e-07\n",
      "W2 relative error: 3.58e-07\n",
      "W3 relative error: 2.79e-08\n",
      "b1 relative error: 6.86e-05\n",
      "b2 relative error: 1.16e-03\n",
      "b3 relative error: 2.35e-09\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, O = 2, 15, 20, 30, 10\n",
    "T = 4\n",
    "X = np.random.randn(N, T, D)\n",
    "Y = np.random.randn(N, O)\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = AdvancedLSTM([H1, H2], input_dim=D, output_dim=O, \n",
    "                       reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, Y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, Y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, train on a small dataset of 50 instances. We use three layers of LSTM and tweak the learning rate and initialization scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 25834.386138\n",
      "(Epoch 0 / 20) train_acc: -24585.993359; val_acc: -22754.584035\n",
      "(Epoch 1 / 20) train_acc: -24553.678901; val_acc: -22722.841330\n",
      "(Epoch 2 / 20) train_acc: -24489.199432; val_acc: -22659.478862\n",
      "(Epoch 3 / 20) train_acc: -24424.312421; val_acc: -22595.890431\n",
      "(Epoch 4 / 20) train_acc: -24359.229251; val_acc: -22532.153400\n",
      "(Epoch 5 / 20) train_acc: -24292.794632; val_acc: -22467.386107\n",
      "(Iteration 11 / 40) loss: 23365.510047\n",
      "(Epoch 6 / 20) train_acc: -24226.185597; val_acc: -22401.898157\n",
      "(Epoch 7 / 20) train_acc: -24154.486765; val_acc: -22331.632446\n",
      "(Epoch 8 / 20) train_acc: -24069.942663; val_acc: -22249.184240\n",
      "(Epoch 9 / 20) train_acc: -23956.483160; val_acc: -22138.375839\n",
      "(Epoch 10 / 20) train_acc: -23767.291472; val_acc: -21953.807456\n",
      "(Iteration 21 / 40) loss: 21446.622670\n",
      "(Epoch 11 / 20) train_acc: -23340.993891; val_acc: -21535.989780\n",
      "(Epoch 12 / 20) train_acc: -22434.520582; val_acc: -20649.555723\n",
      "(Epoch 13 / 20) train_acc: -21410.202070; val_acc: -19648.278429\n",
      "(Epoch 14 / 20) train_acc: -20369.472620; val_acc: -18633.258228\n",
      "(Epoch 15 / 20) train_acc: -19333.098594; val_acc: -17627.971436\n",
      "(Iteration 31 / 40) loss: 20595.584323\n",
      "(Epoch 16 / 20) train_acc: -18320.673507; val_acc: -16642.917560\n",
      "(Epoch 17 / 20) train_acc: -17370.295687; val_acc: -15709.193444\n",
      "(Epoch 18 / 20) train_acc: -16456.554631; val_acc: -14819.026648\n",
      "(Epoch 19 / 20) train_acc: -15593.876870; val_acc: -13980.018860\n",
      "(Epoch 20 / 20) train_acc: -14786.478780; val_acc: -13193.151827\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-2\n",
    "model = AdvancedLSTM([20, 10, 20], input_dim=28,\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train(regress=True)\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "\n",
    "In the file `batchnormlstm/optim.py` we implement the SGD+momentum update rule in the function `sgd_momentum`, then run the following to check the implementation. We should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  8.882347033505819e-09\n",
      "velocity error:  4.269287743278663e-09\n"
     ]
    }
   ],
   "source": [
    "from batchnormlstm.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `batchnormlstm/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check the implementations using the tests below.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  9.524687511038133e-08\n",
      "cache error:  2.6477955807156126e-09\n"
     ]
    }
   ],
   "source": [
    "# Test RMSProp implementation; we should see errors less than 1e-7\n",
    "from batchnormlstm.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  1.1395691798535431e-07\n",
      "v error:  4.208314038113071e-09\n",
      "m error:  4.214963193114416e-09\n"
     ]
    }
   ],
   "source": [
    "# Test Adam implementation; we should see errors around 1e-7 or less\n",
    "from batchnormlstm.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model\n",
    "By combining the features and modifying the rules, we can come up with a model that best fits the data. An example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 5340) loss: 22429.565881\n",
      "(Epoch 0 / 30) train_acc: -22142.952517; val_acc: -22784.897269\n",
      "(Iteration 101 / 5340) loss: 20349.013528\n",
      "(Epoch 1 / 30) train_acc: -18865.138875; val_acc: -19299.204119\n",
      "(Iteration 201 / 5340) loss: 18602.975270\n",
      "(Iteration 301 / 5340) loss: 17323.709701\n",
      "(Epoch 2 / 30) train_acc: -16796.062704; val_acc: -17131.768075\n",
      "(Iteration 401 / 5340) loss: 16288.072421\n",
      "(Iteration 501 / 5340) loss: 15502.713236\n",
      "(Epoch 3 / 30) train_acc: -15196.966181; val_acc: -15526.532841\n",
      "(Iteration 601 / 5340) loss: 14649.562715\n",
      "(Iteration 701 / 5340) loss: 14009.695086\n",
      "(Epoch 4 / 30) train_acc: -14044.081025; val_acc: -14265.049174\n",
      "(Iteration 801 / 5340) loss: 13185.206727\n",
      "(Epoch 5 / 30) train_acc: -13031.742671; val_acc: -13231.504870\n",
      "(Iteration 901 / 5340) loss: 13008.184389\n",
      "(Iteration 1001 / 5340) loss: 12358.698903\n",
      "(Epoch 6 / 30) train_acc: -12082.949525; val_acc: -12363.994974\n",
      "(Iteration 1101 / 5340) loss: 11953.610956\n",
      "(Iteration 1201 / 5340) loss: 11570.680937\n",
      "(Epoch 7 / 30) train_acc: -11386.860170; val_acc: -11617.967127\n",
      "(Iteration 1301 / 5340) loss: 11076.299291\n",
      "(Iteration 1401 / 5340) loss: 10706.953638\n",
      "(Epoch 8 / 30) train_acc: -10756.732469; val_acc: -10982.382432\n",
      "(Iteration 1501 / 5340) loss: 10634.784156\n",
      "(Iteration 1601 / 5340) loss: 10100.037639\n",
      "(Epoch 9 / 30) train_acc: -10167.074312; val_acc: -10428.199456\n",
      "(Iteration 1701 / 5340) loss: 9745.082310\n",
      "(Epoch 10 / 30) train_acc: -9761.301357; val_acc: -9949.255366\n",
      "(Iteration 1801 / 5340) loss: 9740.305786\n",
      "(Iteration 1901 / 5340) loss: 9455.521302\n",
      "(Epoch 11 / 30) train_acc: -9220.368611; val_acc: -9526.771915\n",
      "(Iteration 2001 / 5340) loss: 9318.000634\n",
      "(Iteration 2101 / 5340) loss: 8766.926771\n",
      "(Epoch 12 / 30) train_acc: -8826.712583; val_acc: -9157.506234\n",
      "(Iteration 2201 / 5340) loss: 8883.436453\n",
      "(Iteration 2301 / 5340) loss: 8415.150049\n",
      "(Epoch 13 / 30) train_acc: -8601.044907; val_acc: -8831.214089\n",
      "(Iteration 2401 / 5340) loss: 8337.676423\n",
      "(Epoch 14 / 30) train_acc: -8265.879308; val_acc: -8543.576560\n",
      "(Iteration 2501 / 5340) loss: 8176.022965\n",
      "(Iteration 2601 / 5340) loss: 8134.875731\n",
      "(Epoch 15 / 30) train_acc: -7979.966729; val_acc: -8289.256179\n",
      "(Iteration 2701 / 5340) loss: 8010.672002\n",
      "(Iteration 2801 / 5340) loss: 7786.470749\n",
      "(Epoch 16 / 30) train_acc: -7698.672004; val_acc: -8063.710700\n",
      "(Iteration 2901 / 5340) loss: 7724.126279\n",
      "(Iteration 3001 / 5340) loss: 7620.020365\n",
      "(Epoch 17 / 30) train_acc: -7719.087292; val_acc: -7862.021490\n",
      "(Iteration 3101 / 5340) loss: 7325.542304\n",
      "(Iteration 3201 / 5340) loss: 7464.801944\n",
      "(Epoch 18 / 30) train_acc: -7538.533580; val_acc: -7682.794647\n",
      "(Iteration 3301 / 5340) loss: 7422.965915\n",
      "(Epoch 19 / 30) train_acc: -7322.645703; val_acc: -7522.302291\n",
      "(Iteration 3401 / 5340) loss: 7050.178851\n",
      "(Iteration 3501 / 5340) loss: 6933.501588\n",
      "(Epoch 20 / 30) train_acc: -7133.884757; val_acc: -7380.145795\n",
      "(Iteration 3601 / 5340) loss: 7228.305912\n",
      "(Iteration 3701 / 5340) loss: 6913.018620\n",
      "(Epoch 21 / 30) train_acc: -6962.117160; val_acc: -7251.559253\n",
      "(Iteration 3801 / 5340) loss: 7036.348716\n",
      "(Iteration 3901 / 5340) loss: 6832.206765\n",
      "(Epoch 22 / 30) train_acc: -6784.496720; val_acc: -7135.825761\n",
      "(Iteration 4001 / 5340) loss: 7018.763070\n",
      "(Epoch 23 / 30) train_acc: -6748.284937; val_acc: -7032.677722\n",
      "(Iteration 4101 / 5340) loss: 6842.172335\n",
      "(Iteration 4201 / 5340) loss: 6681.822179\n",
      "(Epoch 24 / 30) train_acc: -6647.404915; val_acc: -6940.317673\n",
      "(Iteration 4301 / 5340) loss: 6580.366273\n",
      "(Iteration 4401 / 5340) loss: 6412.021725\n",
      "(Epoch 25 / 30) train_acc: -6612.512929; val_acc: -6857.909244\n",
      "(Iteration 4501 / 5340) loss: 6587.338017\n",
      "(Iteration 4601 / 5340) loss: 6532.855847\n",
      "(Epoch 26 / 30) train_acc: -6474.088960; val_acc: -6783.350387\n",
      "(Iteration 4701 / 5340) loss: 6441.732856\n",
      "(Iteration 4801 / 5340) loss: 6231.865239\n",
      "(Epoch 27 / 30) train_acc: -6450.675438; val_acc: -6716.356471\n",
      "(Iteration 4901 / 5340) loss: 6263.615659\n",
      "(Epoch 28 / 30) train_acc: -6514.149270; val_acc: -6655.453157\n",
      "(Iteration 5001 / 5340) loss: 6569.669732\n",
      "(Iteration 5101 / 5340) loss: 6196.811852\n",
      "(Epoch 29 / 30) train_acc: -6297.476843; val_acc: -6600.935016\n",
      "(Iteration 5201 / 5340) loss: 6293.660362\n",
      "(Iteration 5301 / 5340) loss: 6276.366139\n",
      "(Epoch 30 / 30) train_acc: -6229.338944; val_acc: -6552.802483\n"
     ]
    }
   ],
   "source": [
    "adv_model = None\n",
    "################################################################################\n",
    "# Train a AdvancedLSTM with modified features and rules. Store the model in    #\n",
    "# adv_model variable.                                                          #\n",
    "################################################################################\n",
    "hidden_dims = [20, 15, 20]\n",
    "adv_model = AdvancedLSTM(hidden_dims, input_dim=28, \n",
    "                         weight_scale=1e-2, dropout=0.95, use_batchnorm=True, reg=3e-5)\n",
    "\n",
    "solver = Solver(adv_model, data,\n",
    "                num_epochs=30, batch_size=100,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-2,\n",
    "                },\n",
    "                verbose=True, print_every=100, \n",
    "                lr_decay = 0.9)\n",
    "\n",
    "solver.train(regress=True)\n",
    "################################################################################\n",
    "#                                                                              #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the final model\n",
    "Run the model on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-6441.067650268412"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test set accuracy: ')\n",
    "solver.check_accuracy(data['X_test'], data['y_test'], regress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
